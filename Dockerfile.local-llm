# WellnessAI backend with local LLM (TinyLlama / BioGPT) – CPU-friendly
# Build: docker build -f Dockerfile.local-llm -t wellness-ai-llm .
# Run:   docker run --rm -p 8000:8000 -e WELLNESS_LLM_MODEL=tinyllama wellness-ai-llm

FROM python:3.11-slim

WORKDIR /app

# Install system deps if needed
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY backend/ backend/
COPY run.py .
COPY "Whoop Data" "Whoop Data/"
RUN mkdir -p models user_data

ENV USE_LOCAL_LLM=true
ENV WELLNESS_LLM_MODEL=tinyllama
ENV HOST=0.0.0.0
ENV PORT=8000

EXPOSE 8000

# Download model at build time (optional – remove next 2 lines to download at first run)
# RUN python -c "import sys; sys.path.insert(0,'.'); from scripts.download_model import download_model; download_model('tinyllama')" || true

CMD ["python", "-m", "uvicorn", "backend.main:app", "--host", "0.0.0.0", "--port", "8000"]
